{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"github_commit_analysis\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../data/full.csv\", inferSchema=True, header=True)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_non_null = df.filter(df.repo.isNotNull())\n",
    "df_grouped = df_non_null.groupBy(\"repo\").agg(F.count(\"*\").alias(\"commit_count\")).orderBy(F.desc(\"commit_count\")).limit(10)\n",
    "\n",
    "df_grouped.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "df_spark = df_non_null.filter(df_non_null.repo == 'apache/spark')\n",
    "# df_spark.cache()\n",
    "df_spark_grouped = df_spark.groupBy(\"author\").agg(F.count(\"*\").alias(\"commit_count\")).orderBy(F.desc(\"commit_count\"))\n",
    "df_spark_grouped_top = df_spark_grouped.limit(1)\n",
    "\n",
    "df_spark_grouped_top.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract,to_timestamp, expr, col, concat_ws, slice, split\n",
    "\n",
    "# 3\n",
    "# Mon Apr 19 20:38:03 2021 +0100\n",
    "date_format = \"MMM d HH:mm:ss yyyy Z\"\n",
    "\n",
    "#df_spark.show(truncate=False)\n",
    "#df_spark_copy = df_spark.limit(20)\n",
    "#df_spark.show(truncate=False)\n",
    "#df_spark_extract = df_spark.withColumn(\"date\", regexp_extract(df_spark[\"date\"], \"(\\\\w{3} \\\\d{1,2} \\\\d{2}:\\\\d{2}:\\\\d{2} \\\\d{4} (\\\\+|\\\\-)\\\\d{4})\", 1))\n",
    "df_spark_extract = df_spark.withColumn(\"date\", concat_ws(\" \", slice(split(df_spark[\"date\"], \" \"), 2, int(1e9))))\n",
    "\n",
    "\n",
    "df_spark_extract.show(truncate=False)\n",
    "# Convertir la colonne 'date' en timestamp\n",
    "df_convert = df_spark_extract.withColumn(\"date\", to_timestamp(col(\"date\"), date_format))\n",
    "#df_convert.show(truncate=False)\n",
    "df_spark_four_year = df_convert.filter((df_convert.date >= expr(\"date_sub(current_date(), 4 * 365)\")))\n",
    "df_spark_four_year.orderBy(F.asc(\"date\")).show(truncate=False)\n",
    "\n",
    "df_spark_four_year_grouped = df_spark_four_year.groupBy(\"author\").agg(F.count(\"*\").alias(\"commit_count\")).orderBy(F.desc(\"commit_count\"))\n",
    " \n",
    "#print(df_time.printSchema())\n",
    "df_spark_four_year_grouped.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stop_word_remover = StopWordsRemover()\n",
    "stop_word_remover.setInputCol(\"words_split\")\n",
    "stop_word_remover.setOutputCol(\"no_stop_words\")\n",
    "\n",
    "df_non_null = df.filter(df.repo.isNotNull()).filter(df.message.isNotNull())\n",
    "df_grouped = df_non_null.withColumn('words_split', F.split(df_non_null.message, \" \"))\n",
    "df_grouped = stop_word_remover.transform(df_grouped)\n",
    "df_grouped = df_grouped.withColumn('word', F.explode(df_grouped.no_stop_words))\n",
    "df_grouped = df_grouped.filter(df_grouped.word != '')\n",
    "df_grouped = df_grouped.groupBy(\"word\").agg(F.count(\"word\").alias(\"word_count\")).orderBy(F.desc(\"word_count\")).limit(10)\n",
    "df_grouped.show(truncate=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
